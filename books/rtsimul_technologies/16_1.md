[16 <--- ](16.md) [   Зміст   ](README.md) [--> 16.2](16_2.md)

## 16.1. INTRODUCTION

The path to fast and accurate computer simulation depends on the objective one pursues. In the case of offline applications, the goal is typically to simulate a highly complex system to study its behavior in detail and to ascertain its performance. An example might involve the design of an expensive large-scale system to be built, in which case it is essential to use simulation for the purpose of exploring whether it meets performance specifications and cost constraints under alternative designs and control mechanisms. In such a realm, reasonably fast simulation time is desirable but not indispensable, and one may be more likely to trade off execution speed for higher fidelity. On the contrary, for online applications, we often refer to *real-time simulation* as a tool for generating efficient models providing predictive capability while an actual system is operating as well as the means to explore alternatives that may be implemented in real time. An example might arise when a system is about to

**389**



**390**                                       Real-Time Simulation Technologies

 

operate in an environment it was never originally designed for; one is then interested in simulating how the system would behave in such an environment, possibly under various implementable alternatives (e.g., adding resources to it or modifying the parametric settings of a controller). On the basis of the results, one might proceed in the new setting, abort, or choose the best possible modification before proceeding. In such cases, obtaining information fast from simulation is not just desirable, but an absolute requirement.

Speeding up the process of information extraction from simulation may be attained in various ways. First, one can use *abstraction* to replace a detailed model by a less detailed one that is still sufficiently accurate to provide useful information in a real-time frame for making decisions such as choosing between two or more alternatives to be implemented. Another approach is to use multiple processors oper- ating in parallel. This is usually referred to as *distributed simulation*, where different parts of the entire simulation code are assigned to different processors. By operating in parallel, *N* such processors have the potential of reducing the time to complete a simulation run by a factor of *N*. However, this is rarely feasible since the execution must be synchronized. Thus, one slow processor may delay the rest either because its output is necessary for them to continue execution or because of inefficiencies in the way the code was distributed. Nonetheless, this approach enables interoperability as well as allowing for geographic distribution of processors and some degree of fault tolerance.1,2

The goal of *concurrent simulation* is to provide, at the completion of a single run, information that would normally require *M* separate simulation runs and to do so with minimal overhead. Thus, suppose that the goal of a real-time simulation appli- cation is to determine how a given performance measure *J*(**θ**) varies as a function of some parameter **θ** in the absence of any analytical expression for *J***(****θ)**. This requires the generation of a simulation run at least once for every value of **θ** of interest. It is easy to see how demanding this process becomes: if **θ** represents options from a discrete set {**θ**1, … , **θ***M*}, we need *M* simulation runs under each of **θ**1, … , **θ***M*. It is even more demanding when the system is stochastic, requiring a large number of simulation runs under a fixed **θ***i* to achieve desired levels of statistical accuracy. Concurrent simulation techniques are designed to accomplish the task of provid- ing estimates of all *J*(**θ**1), … , *J*(**θ***M*) at the end of a *single* simulation run instead of *M* runs. Moreover, if a simulation run under some **θ***i* requires *T* time units, then instead of *M* · *T* total simulation time, a concurrent simulation algorithm is intended

to deliver all *M* estimates in *T* + τ*M* time units where often τ*M* < *T* and certainly *T* + τ*M* << *M* · *T*. In general, **θ** = [θ1, … , θ*N*] is a vector of parameters, in which case concurrent simulation is applied to each element θ*i* over all *M* *i* values of θ*i* that are of interest. Thus, at the end of a single simulation run, we obtain estimates of



*J*(q*i* ),… , *J*(q*i* *i* ) for all *i* = 1,… , *N*



at the same time. To keep notation manageable,



1            *M*

however, in most of the discussion that follows we consider a scalar θ.

It is important to emphasize the difference between distributed and concurrent

simulation. In distributed simulation, the code for a single simulation run is shared over *M* processors. Thus, at the end of the run, a single set of input–output data (equivalently, one estimate of the relationship *J*(θ)) is obtained. In concurrent

simulation, a single processor simulates a system but also concurrently simulates



Concurrent Simulation for Online Optimization                      **391**

 

it under *M* − 1 additional settings. Thus, at the end of the run, we obtain a total of *M* input–output data (equivalently, estimates of *J*(θ1), … , *J*(θ*M*)). If, in addition, *M* processors are available for concurrent simulation, then the speedup resulting from

the concurrent construction is further increased by naturally assigning a processor to each of the *M* threads estimating *J*(θ*i*) for each *i* = 1, … , *M*. The speedup benefit in concurrent simulation comes not from the presence of extra processing capacity

(which also implies a higher cost) but from the efficient sharing of input data over multiple simulations of the same system under different settings. Another way of interpreting it is as a method that addresses the question “What if a given system

operating under a nominal value θ1 were to operate under θ2, … , θ*M* with the same input conditions?” The answer to these *M* − 1 “what if” questions can be obtained without having to reproduce *M* − 1 distinct simulations by “brute force”; instead, it can be deduced by processing data from the nominal simulation so as to track the differences caused by replacing θ1 by θ*i*, *i* = 2, … , *M*. In Monte Carlo simulation for instance, the computational cost of generating random variates from several dis-

tributions is much higher than the cost of performing this difference tracking and constructing state trajectories under multiple settings captured by θ1, … , θ*M*.

In this chapter, we will first review the formalism and theoretical foundations

of concurrent simulation as it applies to the class of discrete event systems (DES), also developed in the works by Cassandras and Lafortune3 and Cassandras and Panayiotou.4 The state space of a typical DES involves at least some discrete com- ponents, giving rise to piecewise constant state trajectories. This can be exploited

to predict changes in an observed state trajectory under some setting θ*i* when the system operates under θ*j* ≠ θ*i*. Formally, this is studied as a general *sample path constructability* problem. In particular, given a DES sample path under a parameter value θ, the problem is to construct multiple sample paths of the system under differ-

ent values *using only information available along the given sample path*. A solution

to this problem can be obtained when the system under consideration satisfies a *con- structability* condition. Unfortunately, this condition is not easily satisfied. However, it is possible to enforce it at some expense. For a large class of DES modeled as Markov chains, there are two techniques known as the *Standard Clock* (SC) method and the *Augmented System Analysis* (ASA), which provide solutions to the construc- tability problem. In a more general setting (where one cannot impose any assump- tions on the stochastic characteristics of the event processes), we will describe the process through which the problem is solved by coupling an observed sample path to multiple concurrently generated sample paths under different settings. This ulti- mately leads to a detailed procedure known as the *Time Warping Algorithm* (TWA). At this point, it is worth emphasizing that concurrent sample path constructabil- ity techniques can be used in two different modes. First, they can be used offline to

obtain estimates of the system performance under different parameters θ1, … , θ*M* from a single simulation run under θ0. A main objective in this case is to reduce the overall simulation time to achieve high speedup. This approach was investigated in

the work by Cassandras and Panayiotou.4 The emphasis of this chapter is on the online use of sample path constructability techniques where the nominal sample

path θ0 is simply the one observed during the operation of the *actual system*. In this case, based on the observed data, the goal is to estimate the system’s performance



**392**                                       Real-Time Simulation Technologies

 

again under a set of hypothetical parameter values θ1, … , θ*M*. In this case, a con- troller can monitor the performance of the system under the different parameters *J*(θ0), … , *J*(θ*M*), and at the end of each epoch, it can switch to the parameter that achieves the best performance. In this setting, another important advantage of con-

current sample path constructability techniques is that they use *common random numbers* (CRN), and as a result, the probability of selecting the parameter that optimizes the performance *J*(θ) is increased. Another important advantage of the

online use of concurrent sample path constructability techniques is that it does not

require any prior knowledge of the underlying distributions of the various events since those are already embedded in the occurrence times of the observed events. This makes the approach applicable even when the distributions are time varying.

We begin the chapter by reviewing the *Stochastic Timed Automaton* (STA) modeling framework for DES in Section 16.2. This is the framework used by most commercial discrete event simulators, thus facilitating the implementation of the con- current simulation techniques to be described. In Section 16.3, we define the general sample path constructability problem. We also review the SC method and the ASA for DES with Markovian structure in their event processes. Then, in Section 16.4, we describe the procedure through which the general constructability problem is solved and derive the aforementioned TWA. In Section 16.5, we describe how the TWA can be used together with optimization schemes (e.g.,5–8 to solve dynamic resource allocation problems (see also works by Panayiotou and Cassandras9,10). The chapter ends with some final thoughts and conclusions.

[16 <--- ](16.md) [   Зміст   ](README.md) [--> 16.2](16_2.md)