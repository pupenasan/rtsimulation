[15.2 <--- ](15_2.md) [   Зміст   ](README.md) [--> 15.4](15_4.md)

## 15.3. PERSISTENT CHALLENGES

Over the past 30 years, the pipedream of distributed real-time simulation has transformed into a fundamental training technology; still, challenges persist. These include long-standing technology issues, questions regarding how to best attain distributed simulation instructional goals, and discussions on how to best employ this distributed real-time SBT.

### 15.3.1 Interoperability

*Interoperability* concerns the interlinking of the many diverse systems involved with distributed simulation. In other words, an interoperable simulation must be able to both pass and interpret data among its federates [16]. Naturally, interoperability was among the first issues distributed SBT developers confronted, and, although great advances have been made, significant technological gaps remain.

#### 15.3.1.1 Software Protocols

Generally speaking, many interoperability gaps result from limitations found in the software protocols used to communicate data among interlinked simulations. Common protocol issues include the following: 

•   Scalability limitations

•   Challenges regarding time synchronization

•   Lack of interoperability among different protocols

•   Lack of true plug-and-play capabilities

•   Lack of support for semantic interoperability

DIS and HLA remain the most common interoperability protocols; yet, they still only achieve moderate ratings of practical relevance: “(3.5 and 3.4 respectively [out of 5.0]), a value which is relatively high, but might be expected to be even higher considering that both standards have been on the market for more than 10 years (HLA) or 15 years (DIS)” [17]. Both DIS and HLA are most notability affected by scalability, plug-and-play, and semantic interoperability problems [12].

More recent protocols attempt to address the difficulties of DIS and HLA; however, the introduction of new protocols, ironically, contributes to another significant challenge—lack of interoperability among protocols. To use simulations that employ different protocols, developers must use special bridges or gateways, which introduce “increased risk, complexity, cost, level of effort, and preparation time into the event” [12, pp. 8–9]. Plus, the ability to reuse models and applications across different protocols is limited.

To help address these issues, the MSCO recently completed a large-scale effort to identify the best way forward for interoperability. The Live Virtual Constructive Architecture Roadmap (LVCAR) study examined technical architectures, business models, and standards and then considered various strategies for improving the state of simulation interoperability [18]. The study produced 13 documents, including an extensive main report and several companion papers that were delivered in 2008. Although formal next steps have not yet been announced, the results of this effort will undoubtedly shape MSCO policy in the years to come, directly affecting which protocols are developed and/or used.

#### **15.1.1.2**  **Domain** **Architectures**

At the component level, various domain architectures have been established to maximize the potential for interoperability of simulations. Such frameworks attempt to create common software platforms for use across the (mainly government) simulation community. For example, created in the early 1990s, the Joint Simulation System (JSIMS) and One Semi-Automated Forces (OneSAF) systems aim to provide common architectures for synthetic environments and computer generated forces, respectively.

New components are commonly added to government domain architectures, and as a result, these packages are routinely upgraded. Regrettably, many simulation practitioners fail to upgrade their simulations, and now a hodgepodge of versions are found across the user community. Moreover, different versions of an architecture do not necessarily support federation. In other words, the variety of versions actually creates a hindrance to interoperability [19].

No clear solution yet exists to this challenge. However, unifying agencies, such as Simulation Interoperability Standards Organization (SISO), attempt to encourage more standardized architecture use throughout the M&S community. SISO, whose roots can be traced back to SIMNET, is an international organization that hosts regular meetings as well as annual Simulation Interoperability Workshops. SISO also maintains interoperability standards and common components packages, such as SEDRIS, an enabling technology that supports the representation of environmental data and the interchange of environmental data sets among distributed simulation systems [20].

Agencies such as SISO can help remove some barriers to interoperability and promote more homogeneous use of common domain architectures. However, for change to truly occur, user communities must participate with standard activities and commit to building a more cohesive sense of community among distributed SBT practitioners [18].

#### 15.3.1.3 Fair Play

Finally, a range of other interoperability issues can be roughly grouped under the concept of “fair play.” *Fair play* means ensuring that no trainee has an unfair advantage because of technical issues outside of the training, such as improved graphics giving an undue visual advantage. Fair play issues often involve *time synchronization* or *model composability*.

Achieving optimal time synchronization (sometimes called “time coherence”) is a complicated, ongoing struggle for software developers. In brief, time synchronization concerns controlling the timing of simulation events so that they are reflected in the proper order at each participating federate [21]. As of the penning of this chapter, no clear mitigation strategy has been established to overcome distributed synchronization issues; however, investigators continue to search for a solution.

*Model composability*, another as-yet unsolved technological conundrum, is concerned with using the data models of a simulation to effectively represent the common virtual environment in which the distributed participants interact [22]. When simply stated, this challenge seems manageable, but the complexity becomes apparent once one considers the array of intricate, heterogeneous systems—often LVC—that must be capable of expressing the same environment, agents, and behaviors. As with synchronization challenges, no clear resolution has yet emerged, and researchers continue to publish extensively, seeking a more optimal solution.

### 15.3.2 Fidelity

Since the early days of simulation, the M&S community has debated the necessary degree of fidelity. Originally, many practitioners believed that the more physical fidelity a simulator offered, the better its learning environment became. Toward that end, early SBT efforts often focused on perfecting the realism of the simulator, so as to mimic the real world as closely as possible. Consider this quotation describing (but not necessarily condoning) early views on simulation:

> The more like the real-world counterpart, the greater is the confidence that performance in the simulator will be equivalent to operational performance and, in the case of training, the greater is the assurance that the simulator will be capable of supporting the learning of the relevant skills . . . Designing a simulator to realistically and comprehensively duplicate a real-world item of equipment or system is a matter of achieving physical and functional correspondence. The characteristics of the human participant can be largely ignored.

**National Research Council [23, p. 27]** 

The idea of *selected fidelity* was articulated in the late 1980s, and it suggests that individual stimuli within a simulation can be more-or-less realistic (depending on the specific task) and still support effective training [24]. Later research led to the notion that *psychological fidelity* was more important than physical fidelity. With psychological fidelity, simulations could be successful without high physical fidelity—so long as the overall fidelity configuration supported the educational goals of the system (e.g., [2,23,25,26]).

Today, effective simulators come in an array of different fidelity levels, and social scientists better understand (although not completely) how to use more limited degrees of fidelity appropriately. Still, the idea that “more fidelity is better” remains entrenched among many practitioners and simulation users, as a recent U.S. Army Research Institute survey discovered [27].

#### 15.3.2.1 Expanding Fidelity into New Domains

In addition to meeting basic fidelity constraints, new requirements call for M&S capabilities in novel application domains. For instance, the U.S. Marine Corps has recently embarked on an effort to deliver pilot-quality SBT to infantry personnel:

> Today, there are cockpit trainers that are so immersive—for both pilot training and evaluation—that the Services and the Federal Aviation Administration (FAA) allow their substitution for much of the actual flying syllabus. Unfortunately, this level of maturity has not been reached for immersive small unit infantry training which necessarily includes an almost limitless variety of localities, environments, and threats.

**NRAC [28, p. 58]** 

Other Defense simulation goals are concerned with replicating the stress, cognitive load, and emotional fatigue found in the real-life combat situations, or refining human social, cultural, and behavioral models to the point where agents act realistically—down to their body language [29]. These new requirements challenge developers, who must extend the supporting technologies. Even more, they challenge instructional and behavioral scientists, who must define the features of each new domain and determine what levels of fidelity support their instruction.

The Defense sector has apportioned substantial resources to meet these challenges, from both the technological and the behavioral science perspectives. For example,

U.S. Joint Forces Command (USJFCOM) recently began the Future Immersive Training Environment (FITE) effort, which seeks to deliver highly realistic infantry training to ensure that the first combat action of personnel is no more difficult or stressful than their last simulated mission. As of mid-2010, FITE investigators had successfully demonstrated a wearable, dismounted immersive simulation system (see [Figure 15.2](#_bookmark91)). The body-worn system provides interconnectivity among users, who interact with a virtual environment through synchronized visual, auditory, olfactory, and even haptic events. If shot, the participant even feels a slight electrical charge so that they know they are “injured” or “dead” [30].

Other military programs seek to more faithfully model humans. For instance, the Human Social Culture Behavior (HSCB) initiative is a Defense-wide effort to “develop a military science base and field technologies that support socio-cultural understanding and human terrain forecasting in intelligence analysis, operations analysis/planning, training, and Joint experimentation” [29, p. 4]. Under this venture, numerous projects are improving the fidelity of human models, cognitive/behavioral models, and the interaction among the behaviors of agents. By increasing the fidelity of such components, next-generation simulations will be able to support unique “nonkinetic” (i.e., noncombat) training, such as the acquisition of cultural competence, which is not presently well supported in SBT.

![image-20220822221411714](E:\san\Технології\моделиров\gitver_rtsimul\books\rtsimul_technologies\media\image-20220822221411714.png)                                                    

**FIGURE 15.2** Camp Lejeune, North Carolina (February 24, 2010). Marines from the 2d Battalion, 8th Marine Regiment, train with the Future Immersive Training Environment (FITE) Joint Capabilities Technology Demonstration (JCTD) virtual reality system in the simulation center at Camp Lejeune, North Carolina. Sponsored by the U.S. Joint Forces Command, with technical management provided by the Office of Naval Research, the FITE JCTD allows an individual wearing a self-contained virtual reality system, with no external tethers and a small joystick mounted on the weapon, to operate in a realistic virtual world displayed in a helmet mounted display. (U.S. Navy photo by John F. Williams/Released.)

These are just two of many Defense efforts designed to expand the range of high- fidelity SBT. These two instances exemplify a trend with respect to SBT fidelity endeavors. Specifically, much of the “low-hanging fruit” has been plucked, leaving instead requirements for nuanced capability improvements (such as precision tuning of character models), expansion into new content domains (such as cultural training), and expansion into new delivery modalities (such as wearable, immersive systems).

### 15.3.3 Instructional Strategies

As discussed in Section 15.2, a number of factors converged through the 1970s to reshape the general approach of practitioners to SBT. The cognitive revolution had finally taken hold in the United States, and constructivist theories were gaining attention. Instructional designers were beginning to apply ISD approaches to simulation curricula, and behavioral researchers were finally invited to help address the negative training potential of simulations.

These factors contributed to the development and widespread use of simulation- specific instructional strategies such as the event-based [31] or scenario-based training approach [3]. Proponents of such event-based strategies first debunked “the widely held belief that just more practice automatically leads to better skills” by demonstrating that unstructured simulation-based practice engenders “rote behaviors and an inflexibility to recognize errors” [32, p. 15]. They then called for the scenarios to be systematically organized around predictable training objectives and employ guided-practice principles [31].

Thanks to these efforts, contemporary SBT is more effective. However, three areas of concern remain with respect to instructional strategies. First, while current practice provides some general recommendations for instructional design, few, if any, specific instructional strategies have been identified that span the entire simulation-based learning process (from design to execution, feedback, and remediation). Second, instructional strategies for unique problem domains, such as improving infantry personnel’s perceptual abilities or more rapidly imbuing novice trainees with sophisticated higher-order cognitive abilities have not been well developed. Third, the impact of many uncommon and/or combined strategies has not yet been well documented [33]. Consequently, SBT instructors are often left to choose their own instructional approach, which contributes to wide variations in training effectiveness and puts a greater task burden on SBT facilitators [34].

The military has funded several projects intended to identify theoretical frameworks for improving the pedagogy (or more accurately, “andragogy”) of its SBT. For example, the Office of Naval Research (ONR) recently sponsored the Next- generation Expeditionary Warfare Intelligent Training (NEW-IT) initiative, which seeks to improve SBT instructional effectiveness, in part, by including automated instructional strategies within distributed SBT software. NEW-IT is scheduled to complete in 2011; however, its investigators already report training performance gains of 26–50% in their empirical field testing [35]. Another ONR effort, called Algorithms Physiologically Derived to Promote Learning Efficiency (APPLE), aims to systematically inform the selection of instructional strategies across a wide range of domains, including distributed SBT [36]. The APPLE effort just began, but it has potential to greatly improve our collective understanding of optimum instructional methods. Programs such as NEW-IT and APPLE promise to improve the state of instructional strategy use within the military SBT community, but many more investigations into methods for improving the effectiveness of SBT are required before a full understanding of SBT instructional best practices is achieved.

### 15.3.4 Instructor Workload

To be effective, SBT systems rely on significant involvement from expert instructors, who often have all-encompassing duties. The workload of instructors becomes even more pronounced in distributed training contexts, where additional human effort is required to configure and initialize system setup, monitor distributed trainees and LVC entities during the exercise, and manage the delivery of distributed postexercise feedback (e.g., [37,38]). This has led some to argue that a good instructor is the primary determinant of the effectiveness of SBT (e.g., [26,23]) or that “simulators without instructors are virtually useless for training” [39, p. 5]. For distributed real-time SBT, these workload dilemmas become especially pronounced during After Action Review (AAR). 

#### 15.3.4.1 Human Effort Required for Distributed AAR

AAR became a formalized military training technique in the 1940s, and it is intended to provide a structured, nonpunitive approach to feedback delivery. Naturally, once the capacity existed, the objective data generated by simulations began to inform AARs, and as simulators have grown in complexity, greater and greater amounts of objective data have became available. With the introduction of SIMNET in the 1980s, the amount of information that could be utilized in the AAR increased dramatically and so too did the workload on instructors [40].

Typically, an instructor must attend to each distributed training “cell.” This means that the instructors, like the trainees, are geographically dispersed and cannot readily respond to all trainees or interact with one another. During a training exercise, instructors are responsible for observing actions of trainees to identify critical details not objectively captured by the simulation. These details may include radio communications that occur among trainees (outside of the simulation itself) or strategic behaviors too subtle for the simulation to analyze [41]. After observing an exercise, instructors typically facilitate the AAR delivery; they assemble the AAR material, find and discuss key points, explain performance outcomes, and lead talks among the trainees [42].

Heavy reliance on instructors in distributed AAR is not merely an issue of individuals’ tasking. It affects the cost of deployment and, depending on the quantity and ability of the facilitators, can limit training effectiveness. An apparent mitigation for these issues involves supporting AAR through improved automated performance capture, analysis, and debrief. Such systems could also serve as collaborative tools for the distributed instructors.

The Services have been working toward such solutions for decades. For instance, the Army Research Institute funded development of the Dismounted Infantry Virtual After Action Review System (DIVAARS), which supports “DVD-like” replay of simulation and provides some data analysis support. A similarly focused effort, called Debriefing Distributed Simulation-Based Exercises (DDSBE), was sponsored by ONR. Other, more particular, AAR systems also exist—from AAR support for analyzing the physical positioning of infantrymen on a live range (e.g., BASE-IT, sponsored by ONR [43]) to interpretation of trainees’ cognitive states through neurophysiological sensors (e.g., AITE, sponsored by ONR [44]).

A number of other AAR tools, both general and specifically targeted, can be readily discovered. On the one hand, the variety of tools gives practitioners a range of options; however, on the other hand, many of these tools operate within narrow domains and software specifications, have been mainly demonstrated in carefully scripted use-case settings, and fail to interoperate with one another. In practice, current AAR technologies rarely provide adequate support for trainers and fail to provide deep diagnosis of performance [41]. Nonetheless, ongoing efforts continue to attempt to remedy these problems.

### 15.3.5 Lack of Effectiveness Assessment

In contrast to the AAR challenge (upon which many researchers and developers are concentrating), few investigators have addressed the question of effectiveness assessment. A recent Army Research Institute report explains:

>  More often than not, the simulators are acquired without knowing their training effectiveness, because no empirical research has been done. The vendors, who manufacture and integrate these devices, do not conduct such research because they are in the business of selling simulators, not research. Occasionally training effectiveness research is conducted after the simulators have been acquired and integrated, but this is narrowly focused on these specific simulators, training specific tasks, in this specific training environment. The research tends to produce no general guidance to the training developer, because of its narrow focus, and because it is conducted on a noninterference basis, making experimental control difficult if not impossible.

**Stewart, Johnson, and Howse [27, p. 3]**

Effectiveness assessments fall roughly into two categories. They may consider specific instances or approaches, as the preceding quotation suggests, or they may be carried out at the enterprise level (i.e., strategy-wide impact assessment). Such strategic impact assessments involve identifying future consequences of a course of action, such as the expected return-on-investment from pursuing large-scale SBT curriculum.

Assessing the impact and value of distributed SBT—at either the project or the enterprise levels—is challenging and costly. Many benefits of distributed SBT, such as avoidance of future errors, are difficult to measure, and simulation analysts lack the ability to conduct upfront impact assessments or communicate their results [45]. In addition, developers and sponsors are often reluctant to spend resources on verifying the effectiveness of a system that they already expect (or at least very much hope) to work.

Although Defense organizations encourage researchers and developers to conduct effectiveness evaluations, such efforts are carried out intermittently. Fortunately, MSCO is leading attempts to increase such testing. MSCO publishes standards for project-level VV&A evaluation, and the office recently completed a strategy-level evaluation of return-on-investment for Defense M&S [46]. Nonetheless, many more researchers will have to follow the example set by MSCO before this gap is mitigated.

### 15.3.6 Lack of Use Outside of the Defense Sector

The roots of distributed simulation can be traced back primarily to the Defense sector [47], and unfortunately, its use has remained largely confined to that community [46,48]. Around 2002, academic papers began routinely appearing asking why commercial industry has yet to embrace distributed simulation (e.g., Ref. [49]). This publishing trend continues to date, and the obstacles to adoption outside of the military sector include the following (for more details see Refs. [48] and [50]):

•   Insufficient integration with commercial off-the-shelf (COTS) simulation packages

•   Technical difficultly (or perceived technical difficulty) of federating systems

•   Inefficiency of synchronization algorithms

•   Bugs and lack of verification in distributed models

•   Overly complex runtime management

•   Perceived lack of practical return-on-investment

•   Too much functionality in existing distributed packages not relevant for industry

The dearth of effectiveness testing also contributes to the reluctance of some to embrace distributed SBT. As Randall Gibson explains:

> I would have to conclude that the simulation community is not doing an adequate job of selling simulation successes. Too often I see management deciding *not* to commit to simulation for projects where it could be a significant benefit.

**Gibson et al. [45, p. 2027]**

In short, money is the major driver in business, and most businesses fail to see sufficient return-on-investment for their use of distributed SBT. Additionally, most commercial practitioners claim to be interested in low-cost, throw-away COTS tools, instead of more robust enterprise systems. However, current COTS simulation tools do not adequately support distributed simulation applications, and most industry practitioners are not willing to pay much more than a 10% increase in cost to upgrade current technologies [48].

Recently, several pilot projects investigated the benefits of distributed SBT for businesses such as car manufacturing (see the work by Boer [51]). These studies help advance the cause of distributed SBT for business. However, greater numbers of use cases and return-on-investment analyses are required. A 2008 survey of simulation practitioners also suggests that more “success stories” are necessary to overcome industry representatives’ “psychological barriers” to accepting distributed SBT. Other responses in this survey indicate that “ready and robust solutions” and “technological advances” must also be made before the commercial sector fully embraces distributed SBT [17].

[15.2 <--- ](15_2.md) [   Зміст   ](README.md) [--> 15.4](15_4.md)