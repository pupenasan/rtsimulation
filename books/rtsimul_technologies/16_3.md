[16.2 <--- ](16_2.md) [   Зміст   ](README.md) [--> 16.4](16_4.md)

## 16.3. SAMPLE PATH CONSTRUCTABILITY

​        

​        1        



​        *M*        



​        1        



​        *M*        



Let us consider a DES modeled as a STA and let Q = {θ0, θ1, … , θ*M*} be a finite set containing all possible values that a parameter θ in this model may take. As mentioned earlier, in general, **θ** is a vector, so if **θ** = [θ1, … , θ*N*] and θ*i* can take *M**i* values, then we can write Q = {q1 ,…,q1 0 ,…,q*N* ,…,q*N* *N* } . Moreover, it is possible that Q is



a set of feasible operating policies or design choices that affect the dynamics of the DES. For example, in a G/G/1/θ queueing system, we may consider the capacity θ as a model parameter taking integer values for some set Q1 = {1, … , *K*}. Alternatively,

we may want to study the performance of this DES as a function of different queue-

ing disciplines such as FIFO, Last-In-First-Out (LIFO), and Random; in this case,

Q2 = {FIFO, LIFO, Random}. Yet another example is the simultaneous investigation of the queue capacity and the queueing discipline, and in this case, Q3 = Q1 ´ Q2. Let *J*(θ) denote a performance metric of the system under θ. Since the DES is in general stochastic, let

w(q) = {(*ek* ,*tk* ), *k* = 1, 2,…}

be a sample path of the system when it is operating under θ, where *ek* ∈ *E* is the *k*th event observed in this sample path and *tk* is the associated occurrence time. The corresponding sample performance metric is denoted by *L*[ω(θ)], and we are interested in estimating performance metrics of the form

*J* (q) = *E* [*L* [w(q)]]

Our premise is that there is no closed-form expression for *J*(θ); therefore, we rely on simulation to obtain sample paths ω(θ) from which we can compute *L*[ω(θ)] and ultimately estimate the expected value *E*[*L*[ω(θ)]], typically through

​     

​        *R*        



ˆ      1



​       

​        *R*        



*J* (q) =   å *L* [w*i* (q)]



*i* =1

 

where ω*i*(θ) is the *i*th sample path (equivalently, simulation run or, for online applications, a finite length observation interval). In applications where the objective is to select the value of θ in Q = {θ0, θ1, … , θ*M*} that minimizes *J*(θ), then the “brute force” trial-and-error approach involves a total of (*M* + 1)*·R* simulation runs to estimate performance for each of the *M* + 1 settings and then select the optimal one. For offline

optimization, assuming all event lifetime distributions are known, then this is fea-

sible, although possibly computationally expensive. For online applications, however, where event lifetime distributions may not be known exactly or where event lifetime distributions may be time varying, the brute force approach is infeasible. The goal of

concurrent simulation methods is to extract estimates *J*ˆ(q*i* ), *i* = 1,… , *M* by observing only one sample path under parameter θ0. To study whether this is possible and under

what conditions, we pose the sample path constructability problem as follows:

For a DES under θ0, construct all sample paths ω(θ1), … , ω(θ*M*) given a realization of lifetime sequences *V*1, … , *VN* and the sample path ω(θ0).



**396**                                       Real-Time Simulation Technologies

​                                                                                                                                                                                                                                  

​            *V*1 = {*V*1, 1, *V*1, 2, ...}             *V*N = {*V*N , 1, *V*N , 2, ...}            

​             0            

​             ( 0)            

​             1            

​             ( 1)            

​             *M*            

​             ( *M*)            

​            DES            

​            DES            

​            DES            







 

**FIGURE 16.1** The sample path constructability problem.

 

This is illustrated in Figure 16.1. There is a simple sufficient condition under which the constructability problem can be solved. This condition consists of two parts. First, let {*Xk*(θ)}, *k* = 1, 2, … , be the state sequence observed in a sample path

ω(θ) under θ in which the lifetime sequences are **V***i* = {***V\****i*,1, ***V\****i*,2, …}, *i* = 1, … , *N*. We

refer to this as the *nominal* sample path. Then, for θ*m* ≠ θ, suppose the DES is driven by the same event sequence generated under θ, giving rise to a new state sequence

{*Xk*(θ*m*)}, *k* = 1, 2, … , and sample path ω(θ*m*). We say that ω(θ*m*) is *observable* with respect to ω(θ) if Γ(*Xk*(θ*m*)) ⊆ Γ(*Xk*(θ)) for all *k* = 1, 2, … . Thus, we have the following

*observability condition* (**OB**):

(**OB**) Let ω(θ) be a sample path under θ, and {*Xk*(θ)}, *k* = 0, 1, … , the corresponding state sequence. For θ*m* ≠ θ, let ω(θ*m*) be the sample path generated by the event sequence in ω(θ). Then, for all *k* = 0, 1, … , Γ(*Xk*(θ*m*)) ⊆ Γ(*Xk*(θ)).

By construction, the two sample paths are “coupled” so that the same event sequence drives them both. As the two state sequences subsequently unfold, condition (**OB**) states that every state observed in the nominal sample path is always “richer” in

terms of feasible events. In other words, from the point of view of ω(θ*m*), all feasible events required at state *Xk*(θ*m*) are observable in the corresponding state *Xk*(θ).

The second part of the constructability condition involves the clock values *Yi* of events *i* ∈ *E*. Let *H*(*t*, *z*) be the conditional distribution of *Yi* defined as follows:

*H* (*t*, *z* ) = *P* [*Yi* £ *t* | *Vi* > *z* ]

where *z* is the observed age of the event at the time *H*(*t*, *z*) is evaluated (i.e., the time elapsed since event *i* was last triggered). Given the lifetime distribution *Gi*(*t*), and since *Vi* = *Yi* + *z*, we have

*H* (*t*, *z* ) = *P* [*V* £ *z* + *t* | *V* > *z* ] = *G**i* (*z* + *t* ) *G**i* (*z*)

​     

​        *i*              *i*        



1 *Gi* (*z* )





Concurrent Simulation for Online Optimization                      **397**

 

Since both the distribution *H*(*t*, *z*) and the event age generally depend on the parameter θ, we write *H*(*t*, *zi*,*k*(θ); θ) to denote the cumulative distribution function (cdf) of the clock value of event *i* ∈ *E*, given its age *zi*,*k*(θ). Then, the *constructability*

*condition* (**CO**) is as follows:

(**CO**) Let ω(θ) be a sample path under θ, and {*Xk*(θ)}, *k* = 0, 1, … , the corresponding state sequence. For θ*m* ≠ θ, let ω(θ*m*) be the sample path generated by the event sequence in ω(θ). Then,



 

 

 

and



G (*Xk* (q*m* )) Í G (*Xk* (q))

 

*H* (*t*, *zi*,*k* (q*m* );q*m* ) = *H* (*t*, *zi*,*k* (q);q)





for all

 

 

 

for all



*k* = 0, 1, ¼

 

 

*i* ÎG (*Xk* (q)), *k* = 0, 1,…



 

The first part is simply (**OB**), while the second part imposes a requirement on the event clock distributions. Details on (**CO**) are found in the work by Cassandras and Lafortune,3 but we note here two immediate implications: (1) if the STA has a

Poisson clock structure, then (**OB**) implies (**CO**), and (2) if Γ(*Xk*(θ*m*)) = Γ(*Xk*(θ)) for all *k* = 0, 1, … , then (**CO**) is satisfied. Clearly, (1) simply follows from the memoryless property of the exponential distribution, and it reduces the test of constructabil-

ity to the purely structural condition (**OB**). Regarding (2), it follows from the fact that if feasible event sets are always the same under θ and θ*m*, then all event clock values are also the same.

 

**Example**

 

Consider a G/G/1/*K* queueing system, where *K* = 1, 2, … is the parameter of interest. Under *K* = 3, a sample path ω3 is obtained. We then pose the question whether a sample path ω2, constructed under *K* = 2, is observable with respect to ω3. In what follows,

{*Xk*(*K*)}, *k* = 1, 2, … , denotes the state sequence in a sample path generated with queueing capacity *K* = 1, 2, … . The two state transition diagrams are shown in [Figure 16.2](#_bookmark93).

Since condition (**OB**) must be tested under the assumption that both sample paths are

generated by the exact same input, it is convenient to construct a new system whose state consists of the joint queue lengths of the two original systems. This is referred to as an *augmented system*. As shown in Figure 16.2, we assume both sample paths start from a common state (say 0, for simplicity). On the left part of the augmented system state transition diagram, the two states remain the same. In fact, a difference arises

only when (1) *Xk*(3) = *Xk*(2) = 2 for some *k*, and (2) event *a* occurs. At this point, the state transition gives *Xk* + 1(3) = 3, but *Xk* + 1(2) = 2. From this point on, the two states satisfy *Xk*(3) = *Xk*(2) + 1, as seen in Figure 16.2, until state (1, 0) is visited. At this point, the only feasible event is a departure *d*, bringing the state back to (0, 0).

By inspection of the augmented system state transition diagram, we see that (**OB**) is trivially satisfied for the three states such that *Xk*(3) = *Xk*(2). It is also easily seen to be satisfied at (3, 2) and (2, 1), since Γ(*x*) = {*a*, *d*} for all *x* > 0. The only remaining state

is (1, 0), where we see that Γ(*Xk*(2)) = Γ(0) = {*a*} ⊂ {*a*, *d*} = Γ(1) = Γ(*Xk*(3)), and (**OB**) is satisfied. Thus, sample paths of this system under *K* = 2 are indeed observable with respect to sample paths under *K* = 3.



**398**                                       Real-Time Simulation Technologies

​                                                                                                

​            *a*            

​            *a*            

​            *a*            

​            *a*            

​            0      1      2      3            







 

​                                               

​          *d* *a*          

​          *d* *a*          

​          *d*     *a*          

​          0      1      2          









​                                                  

​          0,0           *d*          

​          *d*          

​          *a*     1,1     *d*          

​          *a*          

​          1,0     *d*          

​          *a*          

​          2,1     *d*     *a*          

​          *a*     3,2          

​          *a*          

​          2,2          



*d*         *d*



**FIGURE 16.2** State transition diagrams for G/G/1/3 and G/G/1/2 along with the corresponding augmented system.

 

Let us now reverse the roles of the two parameter values and corresponding sample paths. Thus, the question is whether a sample path constructed under *K* = 3 is observable with respect to a sample path under *K* = 2. Returning to Figure 16.2, and state (1,

\0) in particular, note that Γ(*Xk*(3)) = Γ(1) = {*a*, *d*} ⊃ {*a*} = Γ(0) = Γ(*Xk*(2)). Thus, the feasible event *d*, needed at state *Xk*(3) = 1 is *unobservable* at the corresponding state *Xk*(2) = 0. Therefore, it is interesting to note that condition (**OB**) is not symmetric with

respect to the role of the parameter chosen as the “nominal” one.

 

As the previous example illustrates, even for DES modeled as Markov chains, condition (**CO**) is not generally satisfied because (**OB**) may fail. However, it is possible to enforce (**OB**) at the expense of some computational cost or the cost of temporarily “suspending” the construction of the sample paths under some values

in {θ1, … , θ*M*}. We will review next two methods that accomplish this goal, the SC method and ASA, before addressing the more general question of solving the

constructability problem for arbitrary DES.

 

**16.1.1**             **S****tandard** **C****loCk** **a****pproaCh**

The SC approach11 applies to stochastic timed automata with a Poisson clock structure

*–*λ *t*

so that all event lifetime distributions are exponential, and we have *Gi*(*t*) = 1 – *e* *i* for

all *i* ∈ *E*. Thus, every event process in such systems is characterized by a single Poisson parameter λ*i*. As already pointed out, the distributional part of (**CO**) is immediately satisfied by the memoryless property of the exponentially distributed event lifetimes.

The observability condition (**OB**) will be forced to be satisfied by choosing a nominal sample path to simulate such that Γ(*x*) = *E* for all states *x*. To accomplish this, let



L(*x* ) =



å l*i*

*i* ÎG(*x* )



(16.7)



 

A property of Markov chains is that the distribution of the triggering event taking place at state *x* is given by



*p* ( *x* ) =  l*i*  ,



*i* ÎG(*x*)                 (16.8)



*i*      L(*x*)

Moreover, exploiting the uniformization of Markov chains, we replace Λ(*x*) by a uniform rate γ ≥ Λ(*x*) common to all *x* so that the additional probability flow



Concurrent Simulation for Online Optimization                      **399**

[γ − Λ(*x*)] at state *x* corresponds to “fictitious events” that leave the state unchanged. In our case, we choose this uniform rate to be the maximal rate over all events:



L = ål*i* *i* Î*E*



(16.9)



 

In the uniformized model, every event is always feasible. However, events *i* such that *i* ∉ Γ(*x*) for the original model leave the state unchanged, and they are ignored. Moreover, the mechanism through which the triggering event at any state is deter-

mined becomes independent of the state. Specifically, in Equation 16.7, we can now set Λ(*x*) = Λ for all *x*. Therefore, the triggering event distribution at any state is simply *pi* = λ*i*/Λ. The fact that all events are permanently feasible in the uniformized model,

combined with the memoryless property of the exponential event lifetime distribu-

tion, immediately implies the constructability condition (**CO**). In short, the way the SC approach gets around the problem of potentially unobservable events is by simulating a system in which *all* events are forced to occur at *all* states, and hence satisfy (**OB**).

Although this results in several fictitious events, that is, events *i* such that i ∉ Γ(*x*), it also makes for a general-purpose methodology within the realm of Poisson event processes.

There is one additional simplification we can make to this scheme. We can generate all interevent times {*V*1, *V*2, …} from the cdf *G*(*t*) = 1 − *e**−t* and then rescale any specific *Vk* through *Vk*(Λ) = *Vk*/Λ for any given Λ. This allows us to define all event

times: assuming the system starts at time zero, we have *t*1 = *V*1, *t*2 = *t*1 + *V*2, and so

on. The resulting sequence is referred to as the SC and can be generated in advance

of any simulation. The specific SC algorithm for constructing sample paths under

{θ0, … , θ*M*} concurrently with the simulation of a nominal system constructed in this manner is as follows:

 

INITIALIZATION: Construct a SC: {*V*1, *V*2, …}, *Vk* ∼ 1 *− e*–t, *t* > 0. FOR EVERY CONSTRUCTED SAMPLE PATH *m* = 0, 1, … , *M*:

\1.    Determine the triggering event *Em* (by sampling from the pmf

*pi* = λ*i*/Λ).

\2.    Update the state: *X*′*m* = *fm*(*Xm*, *Em*)

\3.    Rescale the observed interevent time *V*: *Vm* = *V*/Λ*m*. Return to step 1 for the next event.

The SC concurrent simulation scheme is extremely simple and applies to any Markov chain regardless of its state transition mechanism. Its main drawback is that it is not applicable for online applications where some of the rates of the event lifetime distributions are not known a priori since fictitious events are not observable. Futhermore, for offline applications, several of the events generated at step 1 of the algorithm may be wasted in the sense that they are fictitious and cause no change in the state of one or more of the constructed sample paths.

 

**16.1.2**             **a****ugmented** **S****yStem** **a****nalySiS**

Unlike the SC method, ASA12,13 is based on observing a real sample path under some setting θ0 and accepting the fact that constructability must be satisfied based on



**400**                                       Real-Time Simulation Technologies

 

whatever events this observed sample path provides. Let ω(θ0) denote the observed sample path, and we need to check whether we can use information extracted from ω(θ0) to construct ω(θ*m*) for some θ*m* ≠ θ0. As the observed state sequence {*Xk*(θ0)} unfolds, we can construct the sequence {*Xk*(θ*m*)} as long as Γ(*Xk*(θ*m*)) ⊆ Γ(*Xk*(θ0)). Now, suppose that we encounter a state pair (*X*(θ0), *X*(θ*m*)) such that Γ(*X*(θ*m*)) ⊃ Γ(*X*(θ0)). This means that there is at least one event, say *i* ∈ Γ(*X*(θ*m*)), which is

unobservable at state *X*(θ0). At this point, the construction of ω(θ*m*) must be suspended, since the triggering event at state *X*(θ*m*) cannot be determined. Then, the key idea in ASA is the following: keep the state in ω(θ*m*) fixed at *X*(θ*m*) and keep on observing ω(θ0) *until some new state X*′(θ0) *is entered that satisfies* Γ(*X*(θ*m*)) ⊆ Γ(*X*′(θ0)). The only remaining question is how long ω(θ*m*) must wait in suspension. Note that we do not require “state matching,” that is, we do not require that *X*′(θ0) = *X*(θ*m*) to continue the construction, but the weaker condition of “event matching”: Γ(*X*(θ*m*)) ⊆ Γ(*X*′(θ)).

We will use ℳ*m* to denote the *mode* of ω(θ*m*), which can take on either the special

value ACTIVE or a value from the state space *Xm*. This gives rise to the following

*Event Matching* algorithm:

INITIALIZATION: Set *ℳm* = ACTIVE and *Xm* = *X*0.

WHEN ANY EVENT (say α) IS OBSERVED AND THE STATE IN ω(θ0) BECOMES *X*, FOR *m* = 1, … , *M*:

\1.    If *ℳm* = ACTIVE, update the state: *X*′*m*= *fm*(*Xm*, α) and if Γ(*X*′*m*) ⊃ Γ(*X*) (i.e., observability is violated), set *ℳm* = *X*′*m*.

\2.                If *ℳm* ≠ ACTIVE and Γ(*ℳm*) ⊆ Γ(*X*), set *ℳm* = ACTIVE; if

*ℳm* ≠ ACTIVE, and Γ(*ℳm*) ⊃ Γ(*X*), then continue.

In step 2, the condition *Mm* ≠ ACTIVE and Γ(*Mm*) ⊆ Γ(*X*) implies that ω(θ*m*) was suspended at the time this event occurs, but since the observability condition Γ(*Mm*) ⊆ Γ(*X*) is satisfied, the sample path construction can resume. The validity of the event matching scheme rests on basic properties of Markov pro-

cesses, which enable us to “cut and paste” segments of a given sample path to produce a new sample path that is stochastically equivalent to the original one (for more details, see the work by Cassandras and Lafortune3). Note that no stopping condition is specified, since this is generally application dependent. Also, as in the case of the SC construction, the result of the Event Matching algorithm is a complete sample path; any desired sample function can subsequently be evaluated from it for the purpose of estimating system performance under

all {θ0, θ1, … , θ*M*} of interest. The main drawback of ASA is the need to suspend a sample path construction for a potentially long time interval. This problem is

the counterpart of the drawback we saw for the SC approach, which consists of “wasted random number generations,” however, unlike SC, ASA can construct sample paths under different parameters using information directly observable from the sample path of an actual system. Finally, it is worth pointing out that the Event Matching algorithm can be modified so as to allow for at most one event process that is not Poisson; details of this extension are given in the work by Cassandras and Lafortune.3

[16.2 <--- ](16_2.md) [   Зміст   ](README.md) [--> 16.4](16_4.md)